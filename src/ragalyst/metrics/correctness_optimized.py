"""Correctness Metric optimized with DSPy Labeled Few-Shot."""

import json
from datetime import datetime
from pathlib import Path

import dspy
import matplotlib.pyplot as plt
import pandas as pd
from dspy.teleprompt import LabeledFewShot
from tqdm import tqdm

from ragalyst.metrics.base import BaseMetric


class CorrectnessDspy(dspy.Signature):
    """You will be given a student answer and a ground truth.

    Your task is to evaluate the student answer by comparing it with the ground truth.
    Give your evaluation on a scale of 0.0 to 1.0, where 0.0 means that the answer is completely unrelated to the ground truth, and 1.0 means that the answer is completely accurate and aligns perfectly with the ground truth.

    For instance,
    correctness_score: 0.0: The answer is completely unrelated to the ground truth.
    correctness_score: 0.3: The answer has minor relevance but does not align with the ground truth.
    correctness_score: 0.5: The answer has moderate relevance but contains inaccuracies.
    correctness_score: 0.7: The answer aligns with the reference but has minor errors or omissions.
    correctness_score: 1.0: The answer is completely accurate and aligns perfectly with the ground truth.

    You MUST provide values for 'correctness_score:' in your answer.

    Now here is the student answer and the ground truth.
    """

    response: str = dspy.InputField(description="Answer generated by an LLM")
    reference: str = dspy.InputField(description="Groundtruth answer")
    correctness_score: float = dspy.OutputField(
        description="Correctness score between response and reference"
    )


class CorrectnessOptimizedMetric(BaseMetric):
    """Correctness Metric optimized with DSPy Labeled Few-Shot."""

    def __init__(self, cfg):
        """Initialize an CorrectnessOptimizedMetric instance."""
        super().__init__(cfg)
        self.train_examples = None
        self.correctness = None
        self._init_dspy()

    def _load_datasets(self):
        with open("src/ragalyst/metrics/datasets/sts_train.json", "r") as f:
            self.sts_train = json.load(f)

        with open("src/ragalyst/metrics/datasets/sts_test.json", "r") as f:
            self.sts_test = json.load(f)

    def _build_signature(self):
        correctness = dspy.Predict(CorrectnessDspy)
        correctness.load("src/ragalyst/metrics/dspy_metrics/answer_correctness.json")
        return correctness

    def _init_dspy(self):
        # Step 1: Load datasets
        self._load_datasets()

        # Step 2: Convert STS-B to DSPy examples
        def convert_to_example(data):
            return [
                dspy.Example(
                    response=sample["sentence1"],
                    reference=sample["sentence2"],
                    correctness_score=sample["similarity"],
                ).with_inputs("response", "reference")
                for sample in tqdm(data)
            ]

        self.train_examples = convert_to_example(self.sts_train)

        # Step 3: Configure LLM and signature
        self.correctness = self._build_signature()
        lm = dspy.LM("openai/gpt-4o-mini")
        dspy.configure(lm=lm)

        self._dspy_initialized = True

    def evaluate(
        self,
        question=None,
        answer=None,
        response=None,
        ground_truth=None,
        context=None,
        max_retries=3,
    ) -> float:
        """Evaluate the correctness of an answer given the ground truth."""
        assert ground_truth is not None, "Ground truth cannot be None"
        assert self.correctness is not None, "Correctness metric is not initialized"

        result = self.correctness(response=answer, reference=ground_truth)
        return result.correctness_score

    def validate_metric(self, dspy_metric=None):
        """Validate the metric prompt on the STS-B (semantic textual similarity benchmark) dataset.

        https://gluebenchmark.com/tasks/
        This shows that our LLM based metric and prompt align with human judgment.
        """
        preds = []
        for item in tqdm(
            self.sts_test,
            total=len(self.sts_test),
            desc="Validating Answer Correctness Metric",
        ):
            if dspy_metric:
                pred = dspy_metric(
                    response=item["sentence1"],
                    reference=item["sentence2"],
                ).correctness_score
            else:
                pred = self.evaluate(
                    answer=item["sentence1"],
                    ground_truth=item["sentence2"],
                )
            preds.append(pred)

        # Compare preds with ground_truths
        results = []
        for pred, sample in zip(preds, self.sts_test):
            results.append(
                {
                    "sentence1": sample["sentence1"],
                    "sentence2": sample["sentence2"],
                    "predicted": pred,
                    "ground_truth": sample["similarity"],
                }
            )

        df = pd.DataFrame(results)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if dspy_metric:
            path = None
        else:
            path = f"evaluations/metrics/answer_correctness/base/{self.cfg.metrics.llm_model_name}_{timestamp}.json"

        spearman_correlation, se = self._spearman_correlation_report(
            df, "predicted", "ground_truth", path
        )

        return spearman_correlation, se

    def optimize_with_labeled_few_shot(self):
        """Optimize the correctness metric with DSPy's Labeled Few-Shot examples."""
        assert self.correctness is not None, "Correctness metric must be initialized."
        assert self.train_examples is not None, "Train examples must be initialized."

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        for k in [1, 2, 4, 8, 16, 32, 64, 128]:
            output_dir = Path(
                f"evaluations/metrics/answer_correctness/miprov2_labeledfewshot/{timestamp}/k={k}"
            )
            output_dir.mkdir(parents=True, exist_ok=True)

            lfs_optimizer = LabeledFewShot(k=k)
            lfs_metric = lfs_optimizer.compile(
                self.correctness,
                trainset=self.train_examples,
            )
            spearman_correlation, se = self.validate_metric(lfs_metric)
            lfs_metric.save(output_dir / "prompt.json")

            with open(output_dir / "performance.json", "w") as f:
                json.dump(
                    {
                        "corr": spearman_correlation,
                        "se_approx": se,
                    },
                    f,
                    indent=4,
                )

    def plot_k_ablation(self):
        """Plot the K ablation study results."""
        performance_data = [
            0.8706909753343527,
            0.8822845206444101,
            0.8905028524783956,
            0.894422399404513,
            0.8925261003547403,
            0.8811832468767948,
            0.8805076409825014,
            0.8771657004510064,
        ]
        k_values = [1, 2, 4, 8, 16, 32, 64, 128]

        plt.figure(figsize=(10, 6))
        plt.plot(k_values, performance_data, marker="o")
        plt.title("K-Ablation Study")
        plt.xlabel("K (Number of Shots)")
        plt.ylabel("Spearman Correlation")
        plt.xticks(k_values)
        plt.grid()
        plt.savefig("outputs/k_ablation_study.png")
        plt.show()
