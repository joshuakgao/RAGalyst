"""Answer Correctness Metric using DSPy and LLMs."""

import json
import os
from datetime import datetime
from pathlib import Path

import dspy
import pandas as pd
from dspy.teleprompt import COPRO, MIPROv2
from matplotlib import pyplot as plt
from tqdm import tqdm

from ragalyst.metrics.base import BaseMetric


class CorrectnessMetric(BaseMetric):
    """Answer Correctness Metric using DSPy and LLMs."""

    def __init__(self, cfg):
        """Initialize an CorrectnessMetric instance."""
        super().__init__(cfg)
        self._dspy_initialized = False
        self.train_examples = None
        self.correctness = None

    def _load_datasets(self):
        with open("src/ragalyst/metrics/datasets/sts_train.json", "r") as f:
            self.sts_train = json.load(f)

        with open("src/ragalyst/metrics/datasets/sts_test.json", "r") as f:
            self.sts_test = json.load(f)

    def _build_signature(self):
        class CorrectnessDspy(dspy.Signature):
            """You will be given a student answer and a ground truth.

            Your task is to evaluate the student answer by comparing it with the ground truth.
            Give your evaluation on a scale of 0.0 to 1.0, where 0.0 means that the answer is completely unrelated to the ground truth, and 1.0 means that the answer is completely accurate and aligns perfectly with the ground truth.

            For instance,
            correctness_score: 0.0: The answer is completely unrelated to the ground truth.
            correctness_score: 0.3: The answer has minor relevance but does not align with the ground truth.
            correctness_score: 0.5: The answer has moderate relevance but contains inaccuracies.
            correctness_score: 0.7: The answer aligns with the reference but has minor errors or omissions.
            correctness_score: 1.0: The answer is completely accurate and aligns perfectly with the ground truth.

            You MUST provide values for 'correctness_score:' in your answer.

            Now here is the student answer and the ground truth.
            """

            response: str = dspy.InputField(description="Answer generated by an LLM")
            reference: str = dspy.InputField(description="Groundtruth answer")
            correctness_score: float = dspy.OutputField(
                description="Correctness score between response and reference"
            )

        return CorrectnessDspy

    def _init_dspy(self):
        if self._dspy_initialized:
            return  # already initialized

        # Step 1: Load datasets
        self._load_datasets()

        # Step 2: Convert STS-B to DSPy examples
        def convert_to_example(data):
            return [
                dspy.Example(
                    response=sample["sentence1"],
                    reference=sample["sentence2"],
                    correctness_score=sample["similarity"],
                ).with_inputs("response", "reference")
                for sample in tqdm(data)
            ]

        self.train_examples = convert_to_example(self.sts_train)

        # Step 3: Configure LLM and signature
        signature_cls = self._build_signature()
        self.correctness = dspy.Predict(signature_cls)
        lm = dspy.LM("openai/gpt-4o-mini")
        dspy.configure(lm=lm)

        self._dspy_initialized = True

    def evaluate(
        self,
        question=None,
        answer=None,
        response=None,
        ground_truth=None,
        context=None,
        max_retries=3,
    ) -> float:
        """Evaluate the correctness of an answer given the ground truth."""
        assert answer is not None, "Answer cannot be None"
        assert ground_truth is not None, "Ground truth cannot be None"

        template = f"""
        You will be given a student answer and a ground truth.
        Your task is to evaluate the student answer by comparing it with the ground truth.
        Give your evaluation on a scale of 0.0 to 1.0, where 0.0 means that the answer is completely unrelated to the ground truth, and 1.0 means that the answer is completely accurate and aligns perfectly with the ground truth.

        For instance,
        correctness_score: 0.0: The answer is completely unrelated to the ground truth.
        correctness_score: 0.3: The answer has minor relevance but does not align with the ground truth.
        correctness_score: 0.5: The answer has moderate relevance but contains inaccuracies.
        correctness_score: 0.7: The answer aligns with the reference but has minor errors or omissions.
        correctness_score: 1.0: The answer is completely accurate and aligns perfectly with the ground truth.

        You MUST provide values for 'correctness_score:' in your answer.

        Now here is the student answer and the ground truth.

        Student Answer: {answer}\n
        Ground truth: {ground_truth}\n

        Format the output as a single number as correctness_score: 0.5 for example. Do not produce any other output. output your rating, as a number between 0.0 and 1.0
    """
        for attempt in range(max_retries + 1):
            try:
                return self.extract_score(template, "correctness_score:")
            except Exception as e:
                print(
                    f"[Retry {attempt}/{max_retries}] Error occurred with correctness metric: {e}"
                )
                continue

        return 0.0

    def validate_metric(self, dspy_metric=None):
        """Validate the metric prompt on the STS-B (semantic textual similarity benchmark) dataset.

        https://gluebenchmark.com/tasks/
        This shows that our LLM based metric and prompt align with human judgment.
        """
        self._init_dspy()

        preds = []
        for item in tqdm(
            self.sts_test,
            total=len(self.sts_test),
            desc="Validating Answer Correctness Metric",
        ):
            if dspy_metric:
                pred = dspy_metric(
                    response=item["sentence1"],
                    reference=item["sentence2"],
                ).correctness_score
            else:
                pred = self.evaluate(
                    answer=item["sentence1"],
                    ground_truth=item["sentence2"],
                )
            preds.append(pred)

        # Compare preds with ground_truths
        results = []
        for pred, sample in zip(preds, self.sts_test):
            results.append(
                {
                    "sentence1": sample["sentence1"],
                    "sentence2": sample["sentence2"],
                    "predicted": pred,
                    "ground_truth": sample["similarity"],
                }
            )

        df = pd.DataFrame(results)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if dspy_metric:
            path = None
        else:
            path = f"evaluations/metrics/answer_correctness/base/{self.cfg.metrics.llm_model_name}_{timestamp}.json"

        spearman_correlation, se = self._spearman_correlation_report(
            df, "predicted", "ground_truth", path
        )

        return spearman_correlation, se

    def optimize_with_copro(self):
        """Optimize the correctness metric with DSPy's COPRO."""
        self._init_dspy()

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(f"evaluations/metrics/answer_correctness/copro/{timestamp}")
        output_dir.mkdir(parents=True, exist_ok=True)

        copro_optimizer = COPRO(
            metric=self._compare_metric,
            prompt_model=dspy.LM("openai/gpt-4o-mini"),
        )
        copro_metric = copro_optimizer.compile(
            self.correctness,
            trainset=self.train_examples,
            eval_kwargs={"num_threads": 10, "display_progress": True},
        )
        spearman_correlation, se = self.validate_metric(copro_metric)
        copro_metric.save(output_dir / "prompt.json")

        with open(output_dir / "performance.json", "w") as f:
            json.dump(
                {
                    "corr": spearman_correlation,
                    "se_approx": se,
                },
                f,
                indent=4,
            )

    def optimize_with_miprov2(self):
        """Optimize the correctness metric with DSPy's MIPROv2."""
        self._init_dspy()
        assert self.correctness is not None, "Correctness metric must be initialized."
        assert self.train_examples is not None, "Train examples must be initialized."

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(f"evaluations/metrics/answer_correctness/miprov2/{timestamp}")
        output_dir.mkdir(parents=True, exist_ok=True)

        miprov2_optimizer = MIPROv2(
            metric=self._compare_metric,
            prompt_model=dspy.LM("openai/gpt-4o-mini"),
        )
        miprov2_metric = miprov2_optimizer.compile(
            self.correctness,
            trainset=self.train_examples,
        )
        spearman_correlation, se = self.validate_metric(miprov2_metric)
        miprov2_metric.save(output_dir / "prompt.json")

        with open(output_dir / "performance.json", "w") as f:
            json.dump(
                {
                    "corr": spearman_correlation,
                    "se_approx": se,
                },
                f,
                indent=4,
            )

    def _compare_metric(self, pred_score, human_score, trace=None):
        """Compare two correctness scores and return if they are approximately equal."""
        return round(float(pred_score.correctness_score), 1) == round(
            float(human_score.correctness_score), 1
        )

    def plot(self):
        """Plot the answer correctness results from different models."""
        # Optional: assign colors to models if you want consistent coloring
        model_colors = {
            "gemini-2.5-flash-lite": "royalblue",
            "gemini-2.5-flash": "royalblue",
            "gemini-2.5-pro": "royalblue",
            "gemma-3-27b-it": "gray",
            "Qwen3-30B-A3B-Instruct-2507": "gray",
            "gpt-4.1": "darkgreen",
            "gpt-4.1-mini": "darkgreen",
            "gpt-4.1-nano": "darkgreen",
            "gpt-4o-mini": "darkgreen",
        }

        # Collect data
        model_data = []
        for file in os.listdir("evaluations/metrics/answer_correctness/base/"):
            if file.endswith(".json"):
                file_path = os.path.join(
                    "evaluations/metrics/answer_correctness/base/", file
                )
                model_name = file.split("_")[0]
                with open(file_path, "r") as f:
                    data = json.load(f)["stats"]
                model_data.append(
                    (
                        model_name,
                        data["spearman_correlation"],
                        data["standard_error_approx"],
                    )
                )

        # Sort by Spearman correlation (descending)
        model_data.sort(key=lambda x: x[1], reverse=True)

        # Unpack for plotting
        models = [x[0] for x in model_data]
        answer_correctness_scores = [x[1] for x in model_data]
        # standard_errors = [x[2] for x in model_data]
        colors = [model_colors.get(model, "blue") for model in models]

        # Plot
        plt.figure(figsize=(8, 6))
        bars = plt.bar(
            range(len(answer_correctness_scores)),
            answer_correctness_scores,
            # yerr=standard_errors,
            capsize=5,
            color=colors,
        )
        # Add value labels above bars
        for i, bar in enumerate(bars):
            height = bar.get_height()
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                height + 0.005,  # Slightly above the bar
                f"{height:.3f}",
                ha="center",
                va="bottom",
                fontsize=8,
            )
        plt.xlabel("Model")
        plt.ylabel("Spearman Correlation")
        plt.title("Answer Correctness: Spearman Correlation by Model")
        plt.xticks(range(len(models)), models, rotation=45, ha="right")
        plt.tight_layout()
        plt.savefig("outputs/answer_correctness_plot.png")
