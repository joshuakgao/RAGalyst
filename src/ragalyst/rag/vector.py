"""VectorRag implementation using FAISS as the vector store.

This module provides a concrete subclass of `BaseRag` that integrates with
FAISS for similarity-based retrieval. It uses embeddings generated by a
configured embedder (e.g., OpenAI or Ollama) and supports persistence of
indexes for efficient repeated queries.
"""

from uuid import uuid4

from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from tqdm import tqdm

from ragalyst.rag.base import BaseRag
from ragalyst.utils.cache.cache_path import make_cache_path


class VectorRag(BaseRag):
    """Vector-based retrieval-augmented generation using FAISS."""

    def __init__(self, cfg):
        """Initialize a VectorRag instance.

        This sets up the FAISS index, processes documents into chunks, embeds
        them, and loads or builds a vector store for similarity search.

        Args:
            cfg: A configuration object specifying the embedder, text processor,
                FAISS index type, and other RAG parameters.

        Raises:
            AssertionError: If the text processor type is not "chunk" or no
                chunks are found in the input data.
        """
        assert cfg.data.text_processor.type == "chunk", (
            "VectorRag only supports chunk text processing."
        )

        from ragalyst.module_registry import get_faiss_index

        super().__init__(cfg)

        self.embedder_dim = self.embedder.get_dim()
        self.embeddings_cache_path = make_cache_path(
            purpose="vector_embeddings",
            relevant_cfg_dict={
                "embedder": {
                    "type": cfg.embedder.type,
                    "model_name": cfg.embedder.model_name,
                    "dim": self.embedder_dim,
                },
                "rag": {
                    "type": "vector",
                    "index_type": cfg.rag.index_type,
                },
                "text_processor": {
                    "type": cfg.data.text_processor.type,
                    "chunk_size": cfg.data.text_processor.chunk_size,
                    "chunk_overlap": cfg.data.text_processor.chunk_overlap,
                    "source_column": cfg.data.text_processor.split_by,
                },
                "domain": cfg.domain,
            },
        )
        self.index = get_faiss_index(cfg, dim=self.embedder_dim)
        self.text_processor.process()
        assert self.text_processor.chunks, "No chunks found in the raw data."
        self.documents: list[Document] = self.text_processor.chunks

        # Load vector store from cache if it exists
        if (self.embeddings_cache_path / "index.faiss").exists():
            assert self.embedder.model is not None
            print(f"Loading vector store from cache at '{self.embeddings_cache_path}'")
            self.vector_store = FAISS.load_local(
                folder_path=str(self.embeddings_cache_path),
                embeddings=self.embedder.model,
                allow_dangerous_deserialization=True,
            )

        # Otherwise, build a new FAISS index and save it
        else:
            self.vector_store = FAISS(
                embedding_function=self.embedder.query,
                index=self.index,
                docstore=InMemoryDocstore(),
                index_to_docstore_id={},
            )
            self.uuids = [str(uuid4()) for _ in range(len(self.documents))]

            # Add documents with progress visualization
            batch_size = 1
            for i in tqdm(
                range(0, len(self.documents), batch_size),
                desc="Embedding documents",
                total=len(self.documents) // batch_size,
            ):
                batch = self.documents[i : i + batch_size]
                try:
                    self.vector_store.add_documents(batch)
                except Exception as e:
                    print(batch)
                    print(f"Error adding documents to vector store: {e}")

            self.vector_store.save_local(folder_path=str(self.embeddings_cache_path))

    def search(self, query) -> list[str]:
        """Retrieve contexts relevant to a query using FAISS similarity search.

        Args:
            query (str): The input query string.

        Returns:
            list[str]: A list of retrieved document contents as strings.
        """
        docs = self.vector_store.similarity_search(query=query, k=self.cfg.rag.top_k)

        if self.cfg.rag.order_preserve:
            docs = self._order_preserve(docs)

        # Convert from list of Documents to list of strings
        results = [doc.page_content for doc in docs]

        return results

    def _order_preserve(self, results):
        """Preserve the original order of retrieved documents.

        Args:
            results (list[Document]): List of retrieved documents.

        Returns:
            list[Document]: The documents sorted according to their original
            order in the preprocessed chunks.
        """
        final_docs_map = {}
        for doc in results:
            original_idx = -1
            for i, chunk in enumerate(self.documents):
                if chunk.page_content == doc.page_content:
                    original_idx = i
                    break

            final_docs_map[original_idx] = doc

        # Sort the collected documents by their original index (the dictionary key)
        docs = [doc for _, doc in sorted(final_docs_map.items())]

        return docs
