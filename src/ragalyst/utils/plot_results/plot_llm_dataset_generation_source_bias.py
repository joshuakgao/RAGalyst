"""Plot LLM evaluation results on datasets generated by different LLMs."""

import matplotlib.pyplot as plt
import numpy as np

data = {
    "GPT-4o-mini": {
        "GPT-4o-mini": {
            "answer_correctness": 0.8197999999999962,
            "faithfulness": 0.8725284640863062,
            "answer_relevancy": 0.9507191791627235,
        },
        "Gemini-2.5-flash": {
            "answer_correctness": 0.8522,
            "faithfulness": 0.8587,
            "answer_relevancy": 0.9304,
        },
        "Qwen3-30B-A3B-Instruct-2507": {
            "answer_correctness": 0.8756,
            "faithfulness": 0.8878,
            "answer_relevancy": 0.9289,
        },
    },
    "Gemini-2.5-flash": {
        "GPT-4o-mini": {
            "answer_correctness": 0.8522,
            "faithfulness": 0.8587,
            "answer_relevancy": 0.9304,
        },
        "Gemini-2.5-flash": {
            "answer_correctness": 0.8736,
            "faithfulness": 0.9418,
            "answer_relevancy": 0.9214,
        },
        "Qwen3-30B-A3B-Instruct-2507": {
            "answer_correctness": 0.8594,
            "faithfulness": 0.8791,
            "answer_relevancy": 0.9402,
        },
    },
    "Qwen3-30B-A3B-Instruct-2507": {
        "GPT-4o-mini": {
            "answer_correctness": 0.8268,
            "faithfulness": 0.8583,
            "answer_relevancy": 0.9596,
        },
        "Gemini-2.5-flash": {
            "answer_correctness": 0.8966,
            "faithfulness": 0.9262,
            "answer_relevancy": 0.9092,
        },
        "Qwen3-30B-A3B-Instruct-2507": {
            "answer_correctness": 0.8882,
            "faithfulness": 0.8770,
            "answer_relevancy": 0.9317,
        },
    },
}
# Extract keys
databases = list(data.keys())
models = list(next(iter(data.values())).keys())
metrics = ["answer_correctness", "faithfulness", "answer_relevancy"]

# Assign colors to each database
color_map = {
    "GPT-4o-mini": "tab:green",
    "Gemini-2.5-flash": "tab:blue",
    "Qwen3-30B-A3B-Instruct-2507": "tab:orange",
}

# Plot: one chart per model vertically
fig, axes = plt.subplots(len(models), 1, figsize=(8, 15), sharey=True)

bar_width = 0.25
x = np.arange(len(metrics))  # metrics on x-axis

for idx, model in enumerate(models):
    ax = axes[idx]

    for j, db in enumerate(databases):
        values = [data[db][model][metric] for metric in metrics]
        bars = ax.bar(x + j * bar_width, values, width=bar_width, label=db, color=color_map[db])

        # Add labels on top of bars
        for bar in bars:
            height = bar.get_height()
            ax.text(
                bar.get_x() + bar.get_width() / 2,
                height + 0.003,
                f"{height:.3f}",
                ha="center",
                va="bottom",
                fontsize=9,
            )

    ax.set_title(model, fontsize=14, weight="bold")
    ax.set_xticks(x + bar_width)
    ax.set_xticklabels([m.replace("_", " ").title() for m in metrics], rotation=0)
    ax.set_ylim(0.8, 1.0)
    ax.grid(axis="y", linestyle="--", alpha=0.7)
    # ax.set_xlabel("Metrics")  # Add x-axis label for each subplot

axes[len(models) // 2].set_ylabel("Score")  # optional: label middle subplot

# Place the legend in the first subplot
axes[0].legend(title="Dataset Origin", loc="upper left", fontsize=10, frameon=True, framealpha=0.9)

fig.suptitle("LLM Preference for Self-Generated Datasets", fontsize=16, weight="bold")
plt.tight_layout(rect=(0, 0, 1, 0.95))  # adjust rect to make room for the suptitle

plt.savefig("llm_with_rag_evaluation_vertical.png", dpi=300)
